{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Hr6uu3lMzh0M"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global n_classes, b_count, b_count2, seq1, seq2\n",
    "n_classes = 100\n",
    "b_count = 0\n",
    "b_count2 = 0\n",
    "seq1 = np.asarray(np.zeros((50000)), np.int64)\n",
    "for i in range(50000):\n",
    "  seq1[i]=i\n",
    "rn.shuffle(seq1)\n",
    "\n",
    "seq2 = np.asarray(np.zeros((10000)), np.int64)\n",
    "for i in range(10000):\n",
    "  seq2[i]=i\n",
    "rn.shuffle(seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wyEhNlDlzh0P"
   },
   "outputs": [],
   "source": [
    "_weights = {\n",
    "        'wc1': tf.Variable(tf.truncated_normal([7, 7, 3, 96], stddev=0.01)),\n",
    "        'wc2': tf.Variable(tf.truncated_normal([5, 5, 96, 128], stddev=0.01)),\n",
    "        'wc3': tf.Variable(tf.truncated_normal([3, 3, 128, 256], stddev=0.01)),\n",
    "        'wc4': tf.Variable(tf.truncated_normal([3, 3, 256, 256], stddev=0.01)),\n",
    "        'wc5': tf.Variable(tf.truncated_normal([3, 3, 256, 256], stddev=0.01)),\n",
    "        'wd2': tf.Variable(tf.truncated_normal([512, 512], stddev=0.01)),\n",
    "        'out': tf.Variable(tf.truncated_normal([512, n_classes], stddev=0.01))\n",
    "    }\n",
    "_biases = {\n",
    "        'bc1': tf.Variable(tf.truncated_normal([96], stddev=0.1)),\n",
    "        'bc2': tf.Variable(tf.truncated_normal([128], stddev=0.1)),\n",
    "        'bc3': tf.Variable(tf.truncated_normal([256], stddev=0.1)),\n",
    "        'bc4': tf.Variable(tf.truncated_normal([256], stddev=0.1)),\n",
    "        'bc5': tf.Variable(tf.truncated_normal([256], stddev=0.1)),\n",
    "        'bd2': tf.Variable(tf.truncated_normal([512], stddev=0.1)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_classes], mean=0.1, stddev=0.1))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Z-Oekfd9zh0Q"
   },
   "outputs": [],
   "source": [
    "def relu(x,name=\"activation\"):\n",
    "    return tf.nn.relu(x, name=name)\n",
    "    \n",
    "def conv2d(name, l_input, w, b, s, p, scope):\n",
    "    l_input = tf.nn.conv2d(l_input, w, strides=[1,s,s,1], padding=p, name=name)\n",
    "    l_input = l_input+b\n",
    "    l_input = relu(l_input)\n",
    "\n",
    "    return l_input\n",
    "\n",
    "def max_pool(name, l_input, k, s):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, s, s, 1], padding='VALID', name=name)\n",
    "\n",
    "def norm(l_input, lsize=4, name=\"lrn\"):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "94VSiHqhzh0S"
   },
   "outputs": [],
   "source": [
    "   \n",
    "def alex_net(_X, _dropout, batch_size):\n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'], 3, 'SAME', 'conv1')\n",
    "    pool1 = max_pool('pool1', conv1, k=3,s=2)\n",
    "    conv2 = conv2d('conv2', pool1, _weights['wc2'], _biases['bc2'], 1, 'SAME', 'conv2')\n",
    "    pool2 = max_pool('pool2', conv2, k=3,s=2)\n",
    "    conv3 = conv2d('conv3', pool2, _weights['wc3'], _biases['bc3'], 1, 'SAME', 'conv3')\n",
    "    conv4 = conv2d('conv4', conv3, _weights['wc4'], _biases['bc4'], 1, 'SAME', 'conv4')\n",
    "    conv5 = conv2d('conv5', conv4, _weights['wc5'], _biases['bc5'], 1, 'SAME', 'conv5')\n",
    "    # Find current size of conv5 to fit the requirement of FC1.\n",
    "    sizes = conv5.get_shape().as_list()\n",
    "    neurons =  sizes[1]*sizes[2]*sizes[3]\n",
    "    dense1 = tf.reshape(conv5, [batch_size, neurons]) # Reshape conv5 output to fit dense layer input\n",
    "    wei_den1 = tf.Variable(tf.truncated_normal([neurons, 512], stddev=0.01))\n",
    "    b_den1 = tf.Variable(tf.truncated_normal([512], stddev=0.1))\n",
    "    \n",
    "    dense1 = relu(tf.matmul(dense1, wei_den1) + b_den1, name='fc1') # Relu activation\n",
    "    dd1=tf.nn.dropout(dense1, _dropout)\n",
    "    dense2 = relu(tf.matmul(dd1, _weights['wd2']) + _biases['bd2'], name='fc2') # Relu activation\n",
    "    dd2=tf.nn.dropout(dense2, _dropout)\n",
    "    out = tf.matmul(dd2, _weights['out']) + _biases['out'] # Relu activation\n",
    "\n",
    "    print(\"C1:\",conv1.get_shape())\n",
    "    print(\"P1:\",pool1.get_shape())\n",
    "    print(\"C2:\",conv2.get_shape())\n",
    "    print(\"P2:\",pool2.get_shape())\n",
    "    print(\"C3:\",conv3.get_shape())\n",
    "    print(\"C4:\",conv4.get_shape())\n",
    "    print(\"C5:\",conv5.get_shape())\n",
    "    print(\"d1:\",dense1.get_shape())\n",
    "    print(\"d2:\",dense2.get_shape())  \n",
    "    \n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rotate(img,angle):\n",
    "    M = cv2.getRotationMatrix2D((16,16),angle,1.)\n",
    "    img = cv2.warpAffine(img,M,(32,32))\n",
    "    return img\n",
    "\n",
    "def trans(img,mode):\n",
    "    if(mode == 0):\n",
    "        M = np.float32([[1, 0, 0], [0, 1, 8]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    elif(mode == 1):\n",
    "        M = np.float32([[1, 0, 0], [0, 1, -8]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    elif(mode == 2):\n",
    "        M = np.float32([[1, 0, 8], [0, 1, 0]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    elif(mode == 3):\n",
    "        M = np.float32([[1, 0, -8], [0, 1, 0]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    return img\n",
    "\n",
    "def data_augmentation(img,epoch1):\n",
    "    if epoch1 > 1:\n",
    "        rnd = rn.randint(0,27)\n",
    "        for i in range(len(img)):\n",
    "            im1 = img[i]\n",
    "            if (rnd == 0):\n",
    "                im1 = Rotate(im1,30)\n",
    "            elif (rnd == 1):\n",
    "                im1 = Rotate(im1,60)\n",
    "            elif (rnd == 2):\n",
    "                im1 = Rotate(im1,90)\n",
    "            elif (rnd == 3):\n",
    "                im1 = Rotate(im1,120)\n",
    "            elif (rnd == 4):\n",
    "                im1 = Rotate(im1,-30)\n",
    "            elif (rnd == 5):\n",
    "                im1 = Rotate(im1,-60)\n",
    "            elif (rnd == 6):\n",
    "                im1 = Rotate(im1,-90)                \n",
    "            elif (rnd == 7):\n",
    "                im1 = Rotate(im1,-120)   \n",
    "            elif (rnd == 8):\n",
    "                im1 = cv2.flip(im1,1)\n",
    "            elif (rnd == 9):\n",
    "                im1 = cv2.flip(im1,0)\n",
    "            elif (rnd == 10):\n",
    "                im1 = cv2.flip(im1,-1)\n",
    "            elif (rnd == 11):\n",
    "                im1 = trans(im1,0)\n",
    "            elif (rnd == 12):\n",
    "                im1 = trans(im1,1)\n",
    "            elif (rnd == 13):\n",
    "                im1 = trans(im1,2)\n",
    "            elif (rnd == 13):\n",
    "                im1 = trans(im1,3)\n",
    "            img[i] = im1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tCNUC9U1zh0U"
   },
   "outputs": [],
   "source": [
    "def getvalbatch(data, labels, glen1):\n",
    "    global b_count2, seq2\n",
    "    b_count = b_count2\n",
    "    idx = (b_count*batch_size)%glen1\n",
    "    bx=[]\n",
    "    by=[]\n",
    "    for i in range(batch_size):\n",
    "        idx2 = (idx+i)%glen1\n",
    "        idx2 = seq2[idx2]%glen1\n",
    "        im1 = np.reshape(data[idx2, :], [3,32,32]).transpose(1,2,0)\n",
    "        \n",
    "        im1 = cv2.normalize(im1,None,0,1,cv2.NORM_MINMAX,cv2.CV_32F)\n",
    "        bx.append(im1)\n",
    "        by.append(labels[idx2])\n",
    "    bx = np.reshape(np.asarray(bx), [batch_size, 32, 32, 3])\n",
    "    by = np.asarray(by)\n",
    "    b_count2 +=1\n",
    "    return bx, by\n",
    "\n",
    "def getbatch(data, labels, glen1,epoch1):\n",
    "    global b_count, seq1\n",
    "    idx = (b_count*batch_size)%glen1\n",
    "    bx=[]\n",
    "    by=[]\n",
    "    for i in range(batch_size):\n",
    "        idx2 = (idx+i)%glen1\n",
    "        idx2 = seq1[idx2]%glen1\n",
    "        im1 = np.reshape(data[idx2, :], [3,32,32]).transpose(1,2,0)\n",
    "           \n",
    "        im1 = cv2.normalize(im1,None,0,1,cv2.NORM_MINMAX,cv2.CV_32F)\n",
    "        bx.append(im1)\n",
    "        by.append(labels[idx2])\n",
    "    bx = np.reshape(np.asarray(bx), [batch_size, 32, 32, 3])\n",
    "    bx = data_augmentation(bx,epoch1)\n",
    "    by = np.asarray(by)\n",
    "    b_count +=1\n",
    "    return bx, by\n",
    "\n",
    "def unpickle(file):\n",
    "    # Python 3 version\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    #---------------------------------------------\n",
    "    # Python 2 version\n",
    "    #import cPickle\n",
    "    #with open(file, 'rb') as fo:\n",
    "    #    dict = cPickle.load(fo)\n",
    "    #---------------------------------------------\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "h7AtFd4czh0W"
   },
   "outputs": [],
   "source": [
    "# Training setting\n",
    "batch_size = 300 \n",
    "display_step = 400\n",
    "n_classes = 100 # # of classes\n",
    "dropout = 0.8 # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7LCc7xsQzh0Y"
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)          # Dropout rate to be fed\n",
    "learning_rate = tf.placeholder(tf.float32)      # Learning rate to be fed\n",
    "tX = tf.placeholder(\"float\", [batch_size, 32, 32, 3]) # Training data batch\n",
    "tY = tf.placeholder(\"int64\", [batch_size])            # Training label batch\n",
    "lr = 1e-3                                # Learning rate start\n",
    "print(lr)\n",
    "\n",
    "dict1=unpickle(\"data/train\")\n",
    "data = np.asarray(dict1[b\"data\"], np.float32)\n",
    "labels = dict1[b\"fine_labels\"]\n",
    "glen1 = len(labels)\n",
    "training_iters = glen1 * 1000\n",
    "\n",
    "dict1=unpickle(\"data/test\")\n",
    "data2 = np.asarray(dict1[b\"data\"], np.float32)\n",
    "labels2 = dict1[b\"fine_labels\"]\n",
    "tlen1 = len(data2)\n",
    "\n",
    "print(\"Total labels=%d\"%(max(labels)+1))\n",
    "\n",
    "print(\"Found %d training images & %d test images\"%(glen1, tlen1))\n",
    "\n",
    "pred = alex_net(tX,keep_prob,batch_size)\n",
    "\n",
    "cost = tf.losses.sparse_softmax_cross_entropy(labels=tY, logits=pred)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_prediction=tf.reduce_mean(tf.cast(tf.nn.in_top_k(pred, tY, 5), tf.float32)) #top-k\n",
    "#correct_prediction = tf.equal(tf.argmax(pred, 1), tY)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3CGfQ_Cbzh0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    while step * batch_size < training_iters:\n",
    "        epoch1=np.floor((step*batch_size)/glen1)\n",
    "        bx, by = getbatch(data, labels, glen1,epoch1)\n",
    "        \n",
    "        if (epoch1 > 500) & (lr == 1e-3):\n",
    "            lr = lr / 10\n",
    "            \n",
    "        # Get a batch\n",
    "        sess.run([optimizer],  feed_dict={tX:bx, tY:by, keep_prob: dropout, learning_rate: lr})\n",
    "        \n",
    "        if (step % 15000==1) & (step>15000):\n",
    "            save_path = saver.save(sess, \"tf_alex_model_iter\" + str(step) + \".ckpt\")\n",
    "            print(\"Model saved in file at iteration %d: %s\" % (step*batch_size,save_path))\n",
    "\n",
    "        if step % display_step == 1:\n",
    "            # calculate the loss\n",
    "\n",
    "            loss = sess.run(cost, feed_dict={tX:bx, tY:by, keep_prob: 1.})\n",
    "            acc = sess.run(accuracy, feed_dict={tX:bx, tY:by,keep_prob: 1.})\n",
    "            train_accuracy.append(acc)\n",
    "            acc2 = []\n",
    "            for j in range(int(tlen1 / batch_size)):\n",
    "                tbx, tby = getvalbatch(data2, labels2, tlen1)\n",
    "                acc2.append(sess.run(accuracy, feed_dict={tX:tbx, tY:tby,keep_prob: 1.}))\n",
    "            acc2 = np.mean(acc2)\n",
    "            \n",
    "            print(\"Iter=%d/epoch=%d, Loss=%.6f, Training Accuracy=%.6f, Test Accuracy=%.6f, lr=%f\" \n",
    "                  % (step*batch_size, epoch1 ,loss, acc, acc2, lr))\n",
    "            test_accuracy.append(acc2)\n",
    "\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, \"tf_alex_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "CIFAR10_CNN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
