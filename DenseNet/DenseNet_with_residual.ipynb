{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Hr6uu3lMzh0M"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "import numpy as np\n",
    "import os, pdb\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import time\n",
    "\n",
    "global n_classes,layer_count\n",
    "n_classes = 50\n",
    "layer_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpu_options = tf.GPUOptions(allow_growth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tCNUC9U1zh0U"
   },
   "outputs": [],
   "source": [
    "#==========================================================================\n",
    "#=============Reading data in multithreading manner========================\n",
    "#==========================================================================\n",
    "def read_labeled_image_list(image_list_file, training_img_dir):\n",
    "    \"\"\"Reads a .txt file containing pathes and labeles\n",
    "    Args:\n",
    "       image_list_file: a .txt file with one /path/to/image per line\n",
    "       label: optionally, if set label will be pasted after each line\n",
    "    Returns:\n",
    "       List with all filenames in file image_list_file\n",
    "    \"\"\"\n",
    "    f = open(image_list_file, 'r')\n",
    "    filenames = []\n",
    "    labels = []\n",
    "\n",
    "    for line in f:\n",
    "        filename, label = line[:-1].split(' ')\n",
    "        filename = training_img_dir+filename\n",
    "        filenames.append(filename)\n",
    "        labels.append(int(label))\n",
    "        \n",
    "    return filenames, labels\n",
    "    \n",
    "    \n",
    "def read_images_from_disk(input_queue, size1=256):\n",
    "    \"\"\"Consumes a single filename and label as a ' '-delimited string.\n",
    "    Args:\n",
    "      filename_and_label_tensor: A scalar string tensor.\n",
    "    Returns:\n",
    "      Two tensors: the decoded image, and the string label.\n",
    "    \"\"\"\n",
    "    label = input_queue[1]\n",
    "    fn=input_queue[0]\n",
    "    file_contents = tf.read_file(input_queue[0])\n",
    "    example = tf.image.decode_jpeg(file_contents, channels=3)\n",
    "    \n",
    "    #example = tf.image.decode_png(file_contents, channels=3, name=\"dataset_image\") # png fo rlfw\n",
    "    example=tf.image.resize_images(example, [size1,size1])\n",
    "    return example, label, fn\n",
    "\n",
    "def setup_inputs(sess, filenames, training_img_dir, image_size=256, crop_size=224, isTest=False, batch_size=64):\n",
    "    \n",
    "    # Read each image file\n",
    "    image_list, label_list = read_labeled_image_list(filenames, training_img_dir)\n",
    "\n",
    "    images = tf.cast(image_list, tf.string)\n",
    "    labels = tf.cast(label_list, tf.int64)\n",
    "     # Makes an input queue\n",
    "    if isTest is False:\n",
    "        isShuffle = True\n",
    "        numThr = 4\n",
    "    else:\n",
    "        isShuffle = False\n",
    "        numThr = 1\n",
    "        \n",
    "    input_queue = tf.train.slice_input_producer([images, labels], shuffle=isShuffle)\n",
    "    image, y,fn = read_images_from_disk(input_queue)\n",
    "\n",
    "    channels = 3\n",
    "    image.set_shape([None, None, channels])\n",
    "        \n",
    "    # Crop and other random augmentations\n",
    "    if isTest is False:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        image = tf.random_crop(image, [crop_size, crop_size, 3])\n",
    "    else:\n",
    "        image = tf.image.central_crop(image,(224/256))    \n",
    "        \n",
    "    \n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    \n",
    "    image, y,fn = tf.train.batch([image, y, fn], batch_size=batch_size, capacity=batch_size*3, num_threads=numThr, name='labels_and_images')\n",
    "\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    return image, y, fn, len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x,name=\"activation\"):\n",
    "    return tf.nn.relu(x, name=name)\n",
    "    \n",
    "def conv2d(name, l_input, w, b, s, p):\n",
    "    l_input = tf.nn.conv2d(l_input, w, strides=[1,s,s,1], padding=p, name=name)\n",
    "    l_input = l_input+b\n",
    "\n",
    "    return l_input\n",
    "\n",
    "def batchnorm(conv, isTraining, name='bn'):\n",
    "    return tf.layers.batch_normalization(conv, training=isTraining, name=\"bn\"+name)\n",
    "\n",
    "def initializer(in_filters, out_filters, name, ks=3):\n",
    "    w1 = tf.get_variable(name+\"W\", [ks, ks, in_filters, out_filters], initializer=tf.truncated_normal_initializer())\n",
    "    b1 = tf.get_variable(name+\"B\", [out_filters], initializer=tf.truncated_normal_initializer())\n",
    "    return w1, b1\n",
    "\n",
    "def target_num_channels(in_x,growth_rate,isTraining,name):\n",
    "    target_num_channels = int(in_x.get_shape()[-1]) + 4 * growth_rate\n",
    "    w1,b1 = initializer(int(in_x.get_shape()[-1]),target_num_channels,name+\"target_num_channels\",ks = 1)\n",
    "    x = batchnorm(in_x, isTraining, name=name+'target_num_channels')\n",
    "    x = activation(x)\n",
    "    x = conv2d(name+'r1', x, w1, b1, 1, \"SAME\")\n",
    "    return x\n",
    "    \n",
    "\n",
    "def block_unit(in_x, growth_rate,isTraining, name):\n",
    "    \"\"\"========================bottleneck========================\"\"\"\n",
    "    x = target_num_channels(in_x , growth_rate,isTraining ,name)\n",
    "    channels = int(x.get_shape()[-1])\n",
    "    \n",
    "    w1, b1 = initializer(channels, 4*growth_rate, name+\"block_unit_unit1\", ks=1)\n",
    "    x = batchnorm(x, isTraining, name=name+'bottleneck_unit1')\n",
    "    x = activation(x)\n",
    "    x = conv2d(name+'r1', x, w1, b1, 1, \"SAME\")\n",
    "\n",
    "    w2, b2 = initializer(int(x.get_shape()[-1]), growth_rate, name+\"block_unit_unit2\")\n",
    "    x = batchnorm(x, isTraining, name=name+'bottleneck_unit2')\n",
    "    x = activation(x)\n",
    "    x = conv2d(name+'r2', x, w2, b2, 1, \"SAME\")\n",
    "    \"\"\"========================bottleneck========================\"\"\"\n",
    "    return x\n",
    "    \n",
    "def transition_layer(in_x,isTraining, theta, name):\n",
    "    channels = int(in_x.get_shape()[-1])\n",
    "    x = batchnorm(in_x, isTraining, name=name+'tra')\n",
    "    w1, b1 = initializer(channels, int(channels*theta), name+\"transition_lay\", ks=1)\n",
    "    x = activation(x)\n",
    "    x = conv2d(name+'r1', x, w1, b1, 1, \"SAME\")\n",
    "    x = tf.nn.avg_pool(value=x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name=name+'avg_pool_2x2')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zyjBt-MqVce6"
   },
   "outputs": [],
   "source": [
    "def dense_block(in_x, num_layers ,growth_rate,isTraining, name):\n",
    "    channels = int(in_x.get_shape()[-1])\n",
    "    global layer_count\n",
    "    if layer_count > 0 :\n",
    "        print(\"======================transition=======================\")\n",
    "    with tf.name_scope(\"block1\") as scope:\n",
    "        x = block_unit(in_x, growth_rate, isTraining, name+\"block1\")\n",
    "        x = tf.concat(values=[in_x, x], axis=3, name=name+'stack0')\n",
    "        layer_count += 1\n",
    "        print(\"Denseblock%d with channels %d\"%(layer_count,x.get_shape().as_list()[-1]))\n",
    "        \n",
    "    for i in range(num_layers-1):\n",
    "        name_scope = \"block\" + str(i+2)\n",
    "        layer_count += 1\n",
    "        with tf.name_scope(name_scope) as scope:\n",
    "            out_x = block_unit(x, growth_rate, isTraining, name+ \"block\" +str(i+2))\n",
    "            x = tf.concat(values=[x, out_x], axis=3, name= name +'stack' +str(i+1))\n",
    "            print(\"Denseblock%d with channels %d\"%(layer_count,x.get_shape().as_list()[-1]))\n",
    "    return x\n",
    "\n",
    "      \n",
    "def DenseNet(_X, isTraining,growth_rate):\n",
    "    global n_classes\n",
    "    w1 = tf.get_variable(\"initW\", [7, 7, 3, 96], initializer=tf.truncated_normal_initializer())\n",
    "    b1 = tf.get_variable(\"initB\", [96], initializer=tf.truncated_normal_initializer())\n",
    "    x = conv2d('conv1', _X, w1, b1, 2, \"VALID\")\n",
    "    x = tf.nn.max_pool(x , ksize = [1,3,3,1], strides=[1,2,2,1], padding = \"VALID\")\n",
    "    \n",
    "    num_blocks = 4\n",
    "    num_layers = [4,4,4,4]\n",
    "    for i in range(num_blocks):\n",
    "        x =dense_block(x, num_layers[i],growth_rate, isTraining, \"denb%d\"%(i))\n",
    "        x =transition_layer(x, isTraining, 0.5, \"transb%d\"%(i))\n",
    "        \n",
    "\n",
    "    channels = int(x.get_shape()[-1])\n",
    "    x = batchnorm(x, isTraining, name='FinalBn')    \n",
    "    x = activation(x)\n",
    "    wo, bo=initializer(channels, n_classes, \"FinalOutput\")\n",
    "    x = conv2d('final', x, wo, bo, 1, \"SAME\")\n",
    "    \n",
    "    x = tf.reduce_mean(x, [1,2])\n",
    "    W = tf.get_variable(\"FinalW\", [n_classes, n_classes], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable(\"FinalB\", [n_classes], initializer=tf.truncated_normal_initializer())\n",
    "    \n",
    "    out = tf.matmul(x, W) + b\n",
    "                            \n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "h7AtFd4czh0W"
   },
   "outputs": [],
   "source": [
    "batch_size = 120\n",
    "display_step = 80\n",
    "learning_rate = tf.placeholder(tf.float32)      # Learning rate to be fed\n",
    "lr = 5e-3                          # Learning rate start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7LCc7xsQzh0Y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup the tensorflow...\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "print(\"Preparing the training & validation data...\")\n",
    "train_data, train_labels, filelist1, glen1 = setup_inputs(sess, \"data/train.txt\", \"data/\", batch_size=batch_size)\n",
    "val_data, val_labels, filelist2, tlen1 = setup_inputs(sess, \"data/val.txt\", \"data/\",isTest=True, batch_size=batch_size)\n",
    "\n",
    "max_iter = glen1*100\n",
    "print(\"Preparing the training model with learning rate = %.5f...\" % (lr))\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"DenseNet\") as scope:\n",
    "    pred = DenseNet(train_data, True, 32)\n",
    "    scope.reuse_variables()\n",
    "    valpred = DenseNet(val_data, False, 32)\n",
    "\n",
    "with tf.name_scope('Loss_and_Accuracy'):\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        cost = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=pred)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), train_labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    correct_prediction2 = tf.equal(tf.argmax(valpred,1), val_labels)\n",
    "    accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3CGfQ_Cbzh0c"
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "sess.run(init)\n",
    "step = 0\n",
    "\n",
    "while (step * batch_size) < max_iter:\n",
    "    epoch1=np.floor((step*batch_size)/glen1)\n",
    "    if (((step*batch_size)%glen1 < batch_size) & (lr==5e-3) & (epoch1 >10)):\n",
    "        lr /= 10\n",
    "    if (((step*batch_size)%glen1 < batch_size) & (lr==5e-4) & (epoch1 >20)):\n",
    "        lr /= 5\n",
    "    if (((step*batch_size)%glen1 < batch_size) & (lr==1e-4) & (epoch1 >30)):\n",
    "        lr /= 2\n",
    "    if (((step*batch_size)%glen1 < batch_size) & (lr==5e-5) & (epoch1 >40)):\n",
    "        lr /= 5\n",
    "    if (((step*batch_size)%glen1 < batch_size) & (lr==1e-5) & (epoch1 >60)):\n",
    "        lr /= 10\n",
    "    if (((step*batch_size)%glen1 < batch_size) & (lr==1e-6) & (epoch1 >80)):\n",
    "        lr /= 10         \n",
    "\n",
    "    sess.run(optimizer,  feed_dict={learning_rate: lr})\n",
    "\n",
    "    if step>1 and step % display_step == 1:\n",
    "        # calculate the loss\n",
    "        loss, acc = sess.run([cost, accuracy])\n",
    "        print(\"Iter=%d/epoch=%d, Loss=%.6f, Training Accuracy=%.6f, lr=%f\" % (step*batch_size, epoch1 ,loss, acc,  lr))\n",
    "        train_accuracy.append(acc)        \n",
    "    if step>1 and (step % (display_step*10) == 0):\n",
    "        rounds = tlen1 // batch_size\n",
    "        \n",
    "        valacc=[]\n",
    "        for k in range(rounds):\n",
    "            acc= sess.run(accuracy2)\n",
    "            valacc.append(acc)\n",
    "        print(\"\\nIter=%d/epoch=%d, Validation Accuracy=%.6f\" % (step*batch_size, epoch1 , np.mean(valacc)))\n",
    "        val_accuracy.append(np.mean(valacc))\n",
    "        \n",
    "    step += 1\n",
    "print(\"Optimization Finished!\")\n",
    "save_path = saver.save(sess, \"./tfdensenet_advance_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "ResNet_init.ipynb",
   "provenance": [
    {
     "file_id": "1VL1sK4Hz_IsuhkyQRc8h2SD3w90OF1yp",
     "timestamp": 1526642217104
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
