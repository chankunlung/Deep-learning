{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Hr6uu3lMzh0M"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpu_options = tf.GPUOptions(allow_growth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Hr6uu3lMzh0M"
   },
   "outputs": [],
   "source": [
    "global n_classes, b_count, b_count2, seq1, seq2,abs_path\n",
    "abs_path = \"data/\"\n",
    "ema_gp = []\n",
    "train_phase = True\n",
    "n_classes = 50\n",
    "b_count = 0\n",
    "b_count2 = 0\n",
    "seq1 = np.asarray(np.zeros((63325)), np.int64)\n",
    "for i in range(63325):\n",
    "  seq1[i]=i\n",
    "rn.shuffle(seq1)\n",
    "\n",
    "seq2 = np.asarray(np.zeros((450)), np.int64)\n",
    "for i in range(450):\n",
    "  seq2[i]=i\n",
    "rn.shuffle(seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Z-Oekfd9zh0Q"
   },
   "outputs": [],
   "source": [
    "def activation(x,name=\"activation\"):\n",
    "    return tf.nn.relu(x, name=name)\n",
    "    \n",
    "def conv2d(name, l_input, channels, ks, s, p, tst):\n",
    "    pre_channels=l_input.get_shape().as_list()[3]\n",
    "    w,b =initializer(pre_channels, channels, name+\"conv\", ksize=1)\n",
    "    \n",
    "    l_input = tf.nn.conv2d(l_input, w, strides=[1,s,s,1], padding=p, name=name)\n",
    "    l_input = activation(l_input+b)\n",
    "    l_input = batchnorm(l_input, tst, name)\n",
    "    return l_input\n",
    "\n",
    "def batchnorm(conv, isTraining, name='bn'):\n",
    "    return tf.layers.batch_normalization(conv, training=isTraining, momentum = 0.9, name=\"bn\"+name)\n",
    "\n",
    "def initializer(in_filters, out_filters, name, ksize=3):\n",
    "    w1 = tf.get_variable(name+\"W\", [ksize, ksize, in_filters, out_filters], initializer=tf.glorot_uniform_initializer())\n",
    "    b1 = tf.get_variable(name+\"B\", [out_filters], initializer=tf.glorot_uniform_initializer())\n",
    "    return w1, b1\n",
    "\n",
    "def max_pool(name, l_input, k, s):\n",
    "    return tf.layers.max_pooling2d(l_input, k,s, padding='VALID', name=name+'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "94VSiHqhzh0S"
   },
   "outputs": [],
   "source": [
    "def vgg16(_X, batch_size, tst):\n",
    "    conv1 = conv2d('conv1', _X, 64, 3, 1, 'SAME', tst)\n",
    "    conv2 = conv2d('conv2', conv1, 64, 3, 1, 'SAME', tst)\n",
    "    pool3 = max_pool('pool3', conv2, k=3,s=2) \n",
    "    conv3 = conv2d('conv3', pool3, 128, 3, 1, 'SAME', tst)\n",
    "    conv4 = conv2d('conv4', conv3, 128, 3, 1, 'SAME', tst)\n",
    "    pool5 = max_pool('pool5', conv4, k=3,s=2)\n",
    "    conv6 = conv2d('conv6', pool5, 256, 3, 1, 'SAME', tst)\n",
    "    conv7 = conv2d('conv7', conv6, 256, 3, 1, 'SAME', tst)\n",
    "    pool8 = max_pool('pool8', conv7, k=3,s=2)\n",
    "    conv9 = conv2d('conv9', pool8, 512, 3, 1, 'SAME', tst)\n",
    "    conv10 = conv2d('conv10', conv9, 512, 3, 1, 'SAME', tst)\n",
    "    conv11 = conv2d('conv11', conv10, 512, 3, 1, 'SAME', tst)\n",
    "    pool12 = max_pool('pool12', conv11, k=3,s=2)\n",
    "    conv13 = conv2d('conv13', pool12, 512, 3, 1, 'SAME', tst)\n",
    "    conv14 = conv2d('conv14', conv13, 512, 3, 1, 'SAME', tst)\n",
    "    conv15 = conv2d('conv15', conv14, 512, 3, 1, 'SAME', tst)\n",
    "    pool16 = max_pool('pool16', conv15, k=3,s=2)\n",
    "    \n",
    "    # Find current size of conv5 to fit the requirement of FC1.\n",
    "    sizes = pool16.get_shape().as_list()\n",
    "    neurons =  sizes[1]*sizes[2]*sizes[3]\n",
    "    dense1 = tf.reshape(pool16, [batch_size, neurons]) # Reshape conv5 output to fit dense layer input\n",
    "    wei_den1 = tf.get_variable(\"FC14w\", [neurons, 4096], initializer=tf.glorot_uniform_initializer())\n",
    "    b_den1 = tf.get_variable(\"FC14b\", [4096], initializer=tf.glorot_uniform_initializer())\n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, wei_den1) + b_den1, name='fc1') # Relu activation\n",
    "    \n",
    "    wei_den2 = tf.get_variable(\"FC15w\", [4096, 4096], initializer=tf.glorot_uniform_initializer())\n",
    "    b_den2 = tf.get_variable(\"FC15b\", [4096], initializer=tf.glorot_uniform_initializer())\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, wei_den2) + b_den2, name='fc2') # Relu activation\n",
    "    \n",
    "    wei_den3 = tf.get_variable(\"FC16w\", [4096, n_classes], initializer=tf.glorot_uniform_initializer())\n",
    "    b_den3 = tf.get_variable(\"FC16b\", [n_classes], initializer=tf.glorot_uniform_initializer())\n",
    "    out = tf.matmul(dense2, wei_den3) + b_den3 \n",
    "\n",
    "    print(\"C1:\",conv1.get_shape())\n",
    "    print(\"C2:\",conv2.get_shape())\n",
    "    print(\"P3:\",pool3.get_shape())\n",
    "    print(\"C3:\",conv3.get_shape())\n",
    "    print(\"C4:\",conv4.get_shape())\n",
    "    print(\"P5:\",pool5.get_shape())\n",
    "    print(\"C6:\",conv6.get_shape())\n",
    "    print(\"C7:\",conv7.get_shape())\n",
    "    print(\"P8:\",pool8.get_shape())\n",
    "    print(\"C9:\",conv9.get_shape())\n",
    "    print(\"C10:\",conv10.get_shape())\n",
    "    print(\"C11:\",conv11.get_shape())\n",
    "    print(\"P12:\",pool12.get_shape())\n",
    "    print(\"C13:\",conv13.get_shape())\n",
    "    print(\"C14:\",conv14.get_shape())\n",
    "    print(\"C15:\",conv15.get_shape())\n",
    "    print(\"P16:\",pool16.get_shape())\n",
    "    print(\"d1:\", dense1)\n",
    "    print(\"d2:\", dense2)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropping(image,isVal):\n",
    "    if not isVal:\n",
    "        x,y = np.random.randint(0,32,(2,))\n",
    "        image = image[x:x+224,y:y+224]\n",
    "    else:\n",
    "        x = int(32/2)\n",
    "        y = int(32/2)\n",
    "        image = image[x:x+224,y:y+224]\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rotate(img,angle):\n",
    "    M = cv2.getRotationMatrix2D((112,112),angle,1.)\n",
    "    img = cv2.warpAffine(img,M,(224,224))\n",
    "    return img\n",
    "\n",
    "def trans(img,mode):\n",
    "    if(mode == 0):\n",
    "        M = np.float32([[1, 0, 0], [0, 1, 8]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    elif(mode == 1):\n",
    "        M = np.float32([[1, 0, 0], [0, 1, -8]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    elif(mode == 2):\n",
    "        M = np.float32([[1, 0, 8], [0, 1, 0]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    elif(mode == 3):\n",
    "        M = np.float32([[1, 0, -8], [0, 1, 0]])\n",
    "        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    return img\n",
    "\n",
    "def noisy(img,mode):\n",
    "    #guass\n",
    "    if mode == 0:\n",
    "        row,col,ch= img.shape\n",
    "        mean = 0\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "        gauss = gauss.reshape(row,col,ch)\n",
    "        noisy = img + gauss\n",
    "    elif mode == 1:\n",
    "        row,col,ch = img.shape\n",
    "        s_vs_p = 0.5\n",
    "        amount = 0.004\n",
    "        out = np.copy(img)\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * img.size * s_vs_p)\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "            for i in img.shape]\n",
    "        out[coords] = 1\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* img.size * (1. - s_vs_p))\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "            for i in img.shape]\n",
    "        out[coords] = 0\n",
    "        noisy = out\n",
    "    #possion\n",
    "    elif mode == 2:\n",
    "        vals = len(np.unique(img))\n",
    "        vals = 2 ** np.ceil(np.log2(vals))\n",
    "        noisy = np.random.poisson(img * vals) / float(vals)\n",
    "    #speckle\n",
    "    elif mode == 3:\n",
    "        row,col,ch = img.shape\n",
    "        gauss = np.random.randn(row,col,ch)\n",
    "        gauss = gauss.reshape(row,col,ch)        \n",
    "        noisy = img + img * gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(img,epoch1):\n",
    "    if epoch1 > 1:\n",
    "        rnd = rn.randint(0,32)\n",
    "        if (rnd == 0):\n",
    "            img = Rotate(img,45)\n",
    "        elif (rnd == 1):\n",
    "            img = Rotate(img,90)\n",
    "        elif (rnd == 2):\n",
    "            img = Rotate(img,135)\n",
    "        elif (rnd == 3):\n",
    "            img = Rotate(img,-45)\n",
    "        elif (rnd == 4):\n",
    "            img = Rotate(img,-90)\n",
    "        elif (rnd == 5):\n",
    "            img = Rotate(img,-135)\n",
    "        elif (rnd == 6):\n",
    "            img = cv2.flip(img,1)         \n",
    "        elif (rnd == 7):\n",
    "            img = cv2.flip(img,0)\n",
    "        elif (rnd == 8):\n",
    "            img = cv2.flip(img,-1)\n",
    "        elif (rnd == 9):\n",
    "            img = trans(img,0)\n",
    "        elif (rnd == 10):\n",
    "            img = trans(img,1)\n",
    "        elif (rnd == 11):\n",
    "            img = trans(img,2)\n",
    "        elif (rnd == 12):\n",
    "            img = trans(img,3)\n",
    "        elif (rnd == 13):\n",
    "            img = noisy(img,0)\n",
    "        elif (rnd == 14):\n",
    "            img = noisy(img,1)              \n",
    "        elif (rnd == 15):\n",
    "            img = noisy(img,2)\n",
    "        elif (rnd == 16):\n",
    "            img = noisy(img,3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tCNUC9U1zh0U"
   },
   "outputs": [],
   "source": [
    "def getvalbatch(val_list, batch_size):\n",
    "    global b_count2, seq2\n",
    "    glen1 = len(val_list)\n",
    "    idx = (b_count*batch_size)%glen1\n",
    "    bx=[]\n",
    "    by=[]\n",
    "    for i in range(batch_size):\n",
    "        idx2 = (idx+i)%glen1\n",
    "        idx2 = seq2[idx2]%glen1\n",
    "        file_path,label = val_list[idx2].split(\" \")\n",
    "        im1 = cv2.imread(abs_path + file_path)\n",
    "        im1 = cv2.resize(im1,(256,256))\n",
    "        im1 = cropping(im1,True)\n",
    "        im1 = cv2.normalize(im1,None,0,1,cv2.NORM_MINMAX,cv2.CV_32F)\n",
    "        \n",
    "        bx.append(im1)\n",
    "        by.append(int(label))\n",
    "    b_count2 +=1\n",
    "    return bx, by\n",
    "  \n",
    "def getbatch(train_list, batch_size,epoch1):\n",
    "    global b_count, seq1\n",
    "    glen1 = len(train_list)\n",
    "    idx = (b_count*batch_size)%glen1\n",
    "    bx=[]\n",
    "    by=[]\n",
    "    for i in range(batch_size):\n",
    "        idx2 = (idx+i)%glen1\n",
    "        idx2 = seq1[idx2]%glen1\n",
    "        file_path,label = train_list[idx2].split(\" \")\n",
    "        im1 = cv2.imread(abs_path + file_path)\n",
    "        im1 = cv2.resize(im1,(256,256))\n",
    "        im1 = cropping(im1,False)\n",
    "        im1 = cv2.normalize(im1,None,0,1,cv2.NORM_MINMAX,cv2.CV_32F)\n",
    "        \n",
    "        bx.append(data_augmentation(im1,epoch1))\n",
    "        by.append(int(label))\n",
    "    b_count +=1\n",
    "    return bx, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "h7AtFd4czh0W"
   },
   "outputs": [],
   "source": [
    "# Training setting\n",
    "global train_phase, ema_gp\n",
    "batch_size = 60 \n",
    "display_step = 80\n",
    "n_classes = 50 # # of classes\n",
    "dropout = 0.5# Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7LCc7xsQzh0Y"
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)          # Dropout rate to be fed\n",
    "learning_rate = tf.placeholder(tf.float32)      # Learning rate to be fed\n",
    "\n",
    "tX = tf.placeholder(\"float\", [batch_size, 224, 224, 3]) # Training data batch\n",
    "tY = tf.placeholder(\"int64\", [batch_size])            # Training label batch\n",
    "lr = 7e-5                                 # Learning rate start\n",
    "\n",
    "with open('data/train.txt') as fp:\n",
    "    train_list=fp.readlines()\n",
    "with open('data/val.txt') as fp:\n",
    "    val_list=fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = n_classes\n",
    "glen1 = len(train_list)\n",
    "tlen1 = len(val_list)\n",
    "training_iters = glen1 * 50\n",
    "print(glen1,tlen1,training_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7LCc7xsQzh0Y",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Total labels=%d\"%(labels))\n",
    "print(\"Found %d training images & %d test images\"%(glen1, tlen1))\n",
    "with tf.variable_scope(\"VGG\") as scope:\n",
    "    pred = vgg16(tX,batch_size,True)\n",
    "    scope.reuse_variables()\n",
    "    valpred = vgg16(tX,batch_size,False)\n",
    "   \n",
    "with tf.name_scope(\"Loss_and_Accuracy\"):\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        cost = tf.losses.sparse_softmax_cross_entropy(labels=tY, logits=pred)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tY)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    correct_prediction2 = tf.equal(tf.argmax(valpred, 1), tY)\n",
    "    accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))\n",
    "    \n",
    "    top_k_correct_prediction = tf.reduce_mean(tf.cast(tf.nn.in_top_k(valpred,tY,5),tf.float32))\n",
    "    top_k_accuracy = tf.reduce_mean(tf.cast(top_k_correct_prediction,tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3CGfQ_Cbzh0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "with tf.Session(config = tf.ConfigProto(gpu_options = gpu_options)) as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    while step * batch_size < training_iters:\n",
    "        epoch1=np.floor((step*batch_size)/glen1)\n",
    "\n",
    "        if (((step*batch_size)%glen1 < batch_size) & (lr==7e-5) & (epoch1 > 20)):\n",
    "            lr /= 10\n",
    "        elif (((step*batch_size)%glen1 < batch_size) & (lr==7e-6) & (epoch1 > 30)):\n",
    "            lr /= 10\n",
    "        elif (((step*batch_size)%glen1 < batch_size) & (lr==7e-7) & (epoch1 > 40)):\n",
    "            lr /= 10    \n",
    "            \n",
    "        bx, by = getbatch(train_list, batch_size,epoch1)\n",
    "\n",
    "\n",
    "        # Get a batch\n",
    "        sess.run([optimizer],feed_dict={tX:bx, tY:by, keep_prob: dropout, learning_rate: lr})\n",
    "        \n",
    "        if (step % 15000==1) & (step>15000):\n",
    "            save_path = saver.save(sess, \"./tf_vgg16_model_iter\" + str(step) + \".ckpt\")\n",
    "            print(\"Model saved in file at iteration %d: %s\" % (step*batch_size,save_path))\n",
    "\n",
    "        if step % display_step == 1:\n",
    "            # calculate the loss\n",
    "            loss = sess.run(cost, feed_dict={tX:bx, tY:by,keep_prob:1.})\n",
    "            acc = sess.run(accuracy, feed_dict={tX:bx, tY:by,keep_prob: 1.})\n",
    "            train_accuracy.append(acc)\n",
    "            acc2 = []\n",
    "            top_k = []\n",
    "            for j in range(int(tlen1 / batch_size)):\n",
    "                tbx, tby = getvalbatch(val_list, batch_size)\n",
    "                acc2.append(sess.run(accuracy2, feed_dict={tX:tbx, tY:tby,keep_prob: 1.}))\n",
    "                top_k.append(sess.run(top_k_accuracy,feed_dict = {tX:tbx, tY:tby, keep_prob: 1.}))\n",
    "            top_k = np.mean(top_k)    \n",
    "            acc2 = np.mean(acc2)\n",
    "            test_accuracy.append(acc2)\n",
    "            \n",
    "            print(\"Iter=%d/epoch=%d, Loss=%.6f, Training Accuracy=%.6f, Test Accuracy=%.6f,top_k Accuracy=%.6f,lr=%f\" \n",
    "                  % (step*batch_size, epoch1, loss,acc, acc2,top_k, lr))\n",
    "            train_phase = True\n",
    "\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    save_path = saver.save(sess, \"./tf_vgg16_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "CIFAR10_CNN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
